{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489},{"sourceId":730043,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":556050,"modelId":568613}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nimport glob\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU CONFIGURATION - MAXIMUM P100 UTILIZATION\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # Maximum P100 optimization\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.enabled = True\n    \n    # Enable TF32 on Ampere GPUs (no effect on P100 but future-proof)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Set max split size to avoid fragmentation\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n    \n    print(\"P100 GPU fully optimized for maximum throughput\")\n\n# MODEL ARCHITECTURES\n\nclass MicroArtifactCNN(nn.Module):\n    \"\"\"Specialized for high-frequency synthesis artifacts\"\"\"\n    \n    def __init__(self, dropout=0.6):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Dropout2d(dropout * 0.3)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(dropout * 0.4)\n        )\n\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(dropout * 0.5)\n        )\n        \n        self.conv4 = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(dropout)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.classifier(x)\n        return x\n\n\nclass XceptionDeepfake(nn.Module):\n    \"\"\"EfficientNet-based depthwise separable architecture\"\"\"\n    \n    def __init__(self, dropout=0.6):\n        super().__init__()\n        \n        self.entry = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        \n        self.sep_block1 = self._make_sep_block(64, 128, dropout * 0.4)\n        self.sep_block2 = self._make_sep_block(128, 256, dropout * 0.5)\n        self.sep_block3 = self._make_sep_block(256, 512, dropout * 0.6)\n        \n        self.exit = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1)\n        )\n        \n    def _make_sep_block(self, in_ch, out_ch, dropout):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, in_ch, 3, padding=1, groups=in_ch),\n            nn.BatchNorm2d(in_ch),\n            nn.ReLU(),\n            nn.Conv2d(in_ch, out_ch, 1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(dropout)\n        )\n    \n    def forward(self, x):\n        x = self.entry(x)\n        x = self.sep_block1(x)\n        x = self.sep_block2(x)\n        x = self.sep_block3(x)\n        x = self.exit(x)\n        return x\n\n\nclass ResNet50Deepfake(nn.Module):\n    \"\"\"ResNet architecture tuned for artifact detection\"\"\"\n    \n    def __init__(self, dropout=0.6):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n        \n        self.layer1 = self._make_layer(64, 64, 3, dropout * 0.3)\n        self.layer2 = self._make_layer(64, 128, 4, dropout * 0.4)\n        self.layer3 = self._make_layer(128, 256, 6, dropout * 0.5)\n        self.layer4 = self._make_layer(256, 512, 3, dropout * 0.6)\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1)\n        )\n        \n    def _make_layer(self, in_ch, out_ch, blocks, dropout):\n        layers = []\n        for i in range(blocks):\n            layers.append(nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, padding=1))\n            layers.append(nn.BatchNorm2d(out_ch))\n            layers.append(nn.ReLU())\n            if i == blocks - 1:\n                layers.append(nn.MaxPool2d(2))\n                layers.append(nn.Dropout2d(dropout))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.classifier(x)\n        return x\n\n\nclass EfficientNetB0Deepfake(nn.Module):\n    \"\"\"EfficientNet-B0 for deepfake detection\"\"\"\n    \n    def __init__(self, dropout=0.6):\n        super().__init__()\n        \n        # Stem\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.SiLU()\n        )\n        \n        # MBConv blocks\n        self.blocks = nn.Sequential(\n            self._make_mbconv(32, 16, 1, 1, dropout * 0.2),\n            self._make_mbconv(16, 24, 2, 6, dropout * 0.3),\n            self._make_mbconv(24, 40, 2, 6, dropout * 0.4),\n            self._make_mbconv(40, 80, 3, 6, dropout * 0.5),\n            self._make_mbconv(80, 112, 3, 6, dropout * 0.5),\n            self._make_mbconv(112, 192, 4, 6, dropout * 0.6),\n            self._make_mbconv(192, 320, 1, 6, dropout * 0.6)\n        )\n        \n        # Head\n        self.head = nn.Sequential(\n            nn.Conv2d(320, 1280, 1),\n            nn.BatchNorm2d(1280),\n            nn.SiLU(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Dropout(dropout),\n            nn.Linear(1280, 1)\n        )\n        \n    def _make_mbconv(self, in_ch, out_ch, stride, expand, dropout):\n        mid_ch = in_ch * expand\n        return nn.Sequential(\n            nn.Conv2d(in_ch, mid_ch, 1),\n            nn.BatchNorm2d(mid_ch),\n            nn.SiLU(),\n            nn.Conv2d(mid_ch, mid_ch, 3, stride=stride, padding=1, groups=mid_ch),\n            nn.BatchNorm2d(mid_ch),\n            nn.SiLU(),\n            nn.Conv2d(mid_ch, out_ch, 1),\n            nn.BatchNorm2d(out_ch),\n            nn.Dropout2d(dropout)\n        )\n    \n    def forward(self, x):\n        x = self.stem(x)\n        x = self.blocks(x)\n        x = self.head(x)\n        return x\n\n\n# FACE DETECTION & EXTRACTION WITH SURROUNDING CONTEXT\n\ndef extract_face_with_context(frame, detector, expand_ratio=1.5):\n    \"\"\"\n    Extract face with surrounding region (jawline, neck, hairline)\n    expand_ratio: 1.5 means 50% expansion around detected face\n    \"\"\"\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(80, 80))\n    \n    if len(faces) == 0:\n        return None\n    \n    # Get largest face\n    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n    \n    # Expand to include surrounding context\n    center_x, center_y = x + w // 2, y + h // 2\n    new_w = int(w * expand_ratio)\n    new_h = int(h * expand_ratio)\n    \n    x1 = max(0, center_x - new_w // 2)\n    y1 = max(0, center_y - new_h // 2)\n    x2 = min(frame.shape[1], center_x + new_w // 2)\n    y2 = min(frame.shape[0], center_y + new_h // 2)\n    \n    face_region = frame[y1:y2, x1:x2]\n    \n    return face_region if face_region.size > 0 else None\n\n# VIDEO TO FRAMES EXTRACTION\n\ndef extract_frames_from_video(video_path, detector, max_frames=30, fps_sample=1, face_expansion_ratio=1.5):\n    \"\"\"\n    Extract frames with faces from video\n    max_frames: maximum frames to extract per video\n    fps_sample: sample every N frames to maintain temporal order\n    face_expansion_ratio: expansion ratio for face context\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    frames = []\n    frame_idx = 0\n    extracted = 0\n    \n    # Sample uniformly across video duration\n    sample_interval = max(1, total_frames // max_frames)\n    \n    while cap.isOpened() and extracted < max_frames:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if frame_idx % (sample_interval * fps_sample) == 0:\n            face = extract_face_with_context(frame, detector, expand_ratio=face_expansion_ratio)\n            if face is not None:\n                frames.append(face)\n                extracted += 1\n        \n        frame_idx += 1\n    \n    cap.release()\n    return frames\n\n# DATASET CLASS\n\nclass DeepfakeDataset(Dataset):\n    def __init__(self, frames, labels, transform=None):\n        self.frames = frames\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.frames)\n    \n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        label = self.labels[idx]\n        \n        if self.transform:\n            augmented = self.transform(image=frame)\n            frame = augmented['image']\n        \n        return frame, torch.tensor(label, dtype=torch.float32)\n\n# DATA AUGMENTATION - PRESERVING ARTIFACTS\n\ndef get_transforms(img_size=224):\n    \"\"\"\n    Augmentations that preserve manipulation artifacts\n    \"\"\"\n    train_transform = A.Compose([\n        A.Resize(img_size, img_size),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.3),\n        A.OneOf([\n            A.GaussNoise(var_limit=(5.0, 15.0), p=1.0),\n            A.ISONoise(p=1.0),\n        ], p=0.2),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    val_transform = A.Compose([\n        A.Resize(img_size, img_size),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    return train_transform, val_transform\n\n# TRAINING FUNCTION\ndef train_model(model, train_loader, val_loader, epochs=25, lr=1e-4, model_name=\"model\"):\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n    \n    scaler = torch.cuda.amp.GradScaler()\n    \n    best_val_acc = 0.0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        for frames, labels in pbar:\n            frames, labels = frames.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            \n            optimizer.zero_grad(set_to_none=True)\n            \n            with torch.cuda.amp.autocast():\n                outputs = model(frames).squeeze()\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_loss += loss.item()\n            with torch.no_grad():\n                preds = (torch.sigmoid(outputs) > 0.5).float()\n                train_correct += (preds == labels).sum().item()\n            train_total += labels.size(0)\n            \n            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{train_correct/train_total:.4f}'})\n        \n        avg_train_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        train_losses.append(avg_train_loss)\n        train_accs.append(train_acc)\n        \n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for frames, labels in val_loader:\n                frames, labels = frames.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n                \n                with torch.cuda.amp.autocast():\n                    outputs = model(frames).squeeze()\n                    loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                preds = (torch.sigmoid(outputs) > 0.5).float()\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_acc = val_correct / val_total\n        val_losses.append(avg_val_loss)\n        val_accs.append(val_acc)\n        \n        print(f\"\\nEpoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n              f\"Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n            }, f'{model_name}_best.pth')\n            print(f\"Saved best model: {model_name}_best.pth (Val Acc: {val_acc:.4f})\")\n        \n        scheduler.step()\n        \n        if (epoch + 1) % 5 == 0:\n            torch.cuda.empty_cache()\n    \n    return train_losses, val_losses, train_accs, val_accs, best_val_acc\n# MAIN EXECUTION\n\ndef main():\n    print(\"Searching for dataset directories...\")\n    \n    possible_roots = [\n        \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset\",\n        \"/kaggle/input/*\"\n    ]\n    \n    REAL_DIR = None\n    FAKE_DIR = None\n    \n    for root in possible_roots:\n        real_patterns = [\n            f\"{root}/DFD_original sequences\",\n            f\"{root}/DFD_original_sequences\",\n            f\"{root}/original_sequences\",\n            f\"{root}/Real video\"\n        ]\n        \n        for pattern in real_patterns:\n            matches = glob.glob(pattern)\n            if matches:\n                REAL_DIR = matches[0]\n                print(f\"Found REAL directory: {REAL_DIR}\")\n                break\n        \n        fake_patterns = [\n            f\"{root}/DFD_manipulated sequences/DFD_manipulated sequences\",\n            f\"{root}/DFD_manipulated_sequences/DFD_manipulated_sequences\",\n            f\"{root}/DFD_manipulated sequences\",\n            f\"{root}/Deepfake video\"\n        ]\n        \n        for pattern in fake_patterns:\n            matches = glob.glob(pattern)\n            if matches:\n                FAKE_DIR = matches[0]\n                print(f\"Found FAKE directory: {FAKE_DIR}\")\n                break\n        \n        if REAL_DIR and FAKE_DIR:\n            break\n    \n    if not REAL_DIR or not FAKE_DIR:\n        print(\"\\n  Could not auto-detect dataset directories!\")\n        print(\"\\n Available directories in /kaggle/input:\")\n        for item in glob.glob(\"/kaggle/input/*\"):\n            print(f\"   {item}\")\n            if os.path.isdir(item):\n                for subitem in glob.glob(f\"{item}/*\")[:10]:\n                    print(f\"      └─ {os.path.basename(subitem)}\")\n        \n        raise FileNotFoundError(\n            \"Dataset directories not found. Please check the directory structure above \"\n            \"and update REAL_DIR and FAKE_DIR manually in the code.\"\n        )\n    \n    # =================== CUSTOM SETTINGS ===================\n    FPS_SAMPLE = 3\n    MAX_FRAMES_PER_VIDEO = 200\n    FACE_EXPANSION_RATIO = 1.7\n    TARGET_SIZE = (224, 224)\n    IMG_SIZE = 224\n    BATCH_SIZE = 222\n    EPOCHS = 50\n    # ========================================================\n    \n    print(\" Initializing Haar Cascade face detector...\")\n    detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    print(\"Haar Cascade detector initialized\")\n    \n    # Training on persons 01 through 10\n    TRAIN_ON_SPECIFIC_PERSONS = True\n    PERSON_IDS = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n    \n    all_real_videos = [f for f in os.listdir(REAL_DIR) if f.endswith('.mp4')]\n    all_fake_videos = [f for f in os.listdir(FAKE_DIR) if f.endswith('.mp4')]\n    \n    if TRAIN_ON_SPECIFIC_PERSONS:\n        print(f\"  Training on Person IDs: {', '.join(PERSON_IDS)}\")\n        real_videos = [os.path.join(REAL_DIR, f) for f in all_real_videos \n                       if any(f.startswith(f'{pid}_') for pid in PERSON_IDS)]\n        fake_videos = [os.path.join(FAKE_DIR, f) for f in all_fake_videos \n                       if any(f.startswith(f'{pid}_') for pid in PERSON_IDS)]\n    else:\n        print(f\"  Training on ALL person IDs\")\n        real_videos = [os.path.join(REAL_DIR, f) for f in all_real_videos]\n        fake_videos = [os.path.join(FAKE_DIR, f) for f in all_fake_videos]\n    \n    print(f\"\\nDataset Statistics:\")\n    print(f\"   Total videos in REAL dir: {len(all_real_videos)}\")\n    print(f\"   Total videos in FAKE dir: {len(all_fake_videos)}\")\n    if TRAIN_ON_SPECIFIC_PERSONS:\n        print(f\"  Filtered REAL videos (Persons {', '.join(PERSON_IDS)}): {len(real_videos)}\")\n        print(f\"  Filtered FAKE videos (Persons {', '.join(PERSON_IDS)}): {len(fake_videos)}\")\n    else:\n        print(f\"  Using ALL REAL videos: {len(real_videos)}\")\n        print(f\"  Using ALL FAKE videos: {len(fake_videos)}\")\n    \n    assert len(real_videos) > 0, \" No real videos found! Check video naming convention.\"\n    assert len(fake_videos) > 0, \" No fake videos found! Check video naming convention.\"\n    \n    print(\"\\n Extracting frames from videos using Haar Cascade...\")\n    all_frames = []\n    all_labels = []\n    \n    # Real videos\n    for video_path in tqdm(real_videos, desc=\"Processing REAL videos\"):\n        frames = extract_frames_from_video(video_path, detector, MAX_FRAMES_PER_VIDEO, FPS_SAMPLE, FACE_EXPANSION_RATIO)\n        frames = [cv2.resize(f, TARGET_SIZE) for f in frames if f is not None]\n        all_frames.extend(frames)\n        all_labels.extend([0] * len(frames))\n    \n    # Fake videos\n    for video_path in tqdm(fake_videos, desc=\"Processing FAKE videos\"):\n        frames = extract_frames_from_video(video_path, detector, MAX_FRAMES_PER_VIDEO, FPS_SAMPLE, FACE_EXPANSION_RATIO)\n        frames = [cv2.resize(f, TARGET_SIZE) for f in frames if f is not None]\n        all_frames.extend(frames)\n        all_labels.extend([1] * len(frames++++++++++++++++++++++++++++++++++++++++++d\n    \n    # =================== DATA BALANCING ===================\n    real_count = sum(1 for l in all_labels if l == 0)\n    fake_count = sum(1 for l in all_labels if l == 1)\n    min_count = min(real_count, fake_count)\n    \n    print(f\"\\n DATA BALANCING ACTIVE\")\n    print(f\"   Before: Real={real_count}, Fake={fake_count} (Imbalance ratio: {max(real_count, fake_count)/min_count:.2f}:1)\")\n    print(f\"   Balancing to: {min_count} samples per class...\")\n    \n    real_indices = [i for i, l in enumerate(all_labels) if l == 0]\n    fake_indices = [i for i, l in enumerate(all_labels) if l == 1]\n    \n    np.random.seed(42)\n    np.random.shuffle(real_indices)\n    np.random.shuffle(fake_indices)\n    \n    real_indices = real_indices[:min_count]\n    fake_indices = fake_indices[:min_count]\n    balanced_indices = real_indices + fake_indices\n    \n    all_frames = [all_frames[i] for i in balanced_indices]\n    all_labels = [all_labels[i] for i in balanced_indices]\n    \n    final_real = sum(1 for l in all_labels if l == 0)\n    final_fake = sum(1 for l in all_labels if l == 1)\n    print(f\"   After: Real={final_real}, Fake={final_fake} (Perfect 1:1 balance)\")\n    \n    train_frames, val_frames, train_labels, val_labels = train_test_split(\n        all_frames, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n    )\n    \n    train_transform, val_transform = get_transforms(IMG_SIZE)\n    \n    train_dataset = DeepfakeDataset(train_frames, train_labels, train_transform)\n    val_dataset = DeepfakeDataset(val_frames, val_labels, val_transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                              num_workers=4, pin_memory=True, prefetch_factor=3, persistent_workers=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                            num_workers=4, pin_memory=True, prefetch_factor=3, persistent_workers=True)\n    \n    models = {\n        'MicroArtifactCNN': MicroArtifactCNN(dropout=0.5),\n        'XceptionDeepfake': XceptionDeepfake(dropout=0.5),\n        'ResNet50Deepfake': ResNet50Deepfake(dropout=0.5),\n        'EfficientNetB0': EfficientNetB0Deepfake(dropout=0.5)\n    }\n    \n    results = {}\n    \n    for model_name, model in models.items():\n        print(f\"\\n{'='*70}\")\n        print(f\" Training {model_name}\")\n        print(f\"{'='*70}\")\n        \n        model = model.to(device)\n        train_losses, val_losses, train_accs, val_accs, best_acc = train_model(\n            model, train_loader, val_loader, \n            epochs=EPOCHS, \n            lr=1e-4,\n            model_name=model_name\n        )\n        \n        results[model_name] = {\n            'best_val_acc': best_acc,\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'train_accs': train_accs,\n            'val_accs': val_accs\n        }\n        \n        print(f\"\\n {model_name} completed - Best Val Acc: {best_acc:.4f}\")\n    \n    print(f\"\\n{'='*70}\")\n    print(\"TRAINING SUMMARY\")\n    print(f\"{'='*70}\")\n    for model_name, res in results.items():\n        print(f\"{model_name}: Best Val Acc = {res['best_val_acc']:.4f}\")\n    \n    print(\"\\n All models saved as .pth files (PyTorch format)\")\n    print(\" Models are ready for fine-tuning and ensemble inference\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T11:04:18.578201Z","iopub.execute_input":"2026-01-24T11:04:18.578500Z","iopub.status.idle":"2026-01-24T16:48:23.750252Z","shell.execute_reply.started":"2026-01-24T11:04:18.578474Z","shell.execute_reply":"2026-01-24T16:48:23.749553Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\nVRAM: 17.06 GB\nP100 GPU fully optimized for maximum throughput\nSearching for dataset directories...\nFound REAL directory: /kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\nFound FAKE directory: /kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\n Initializing Haar Cascade face detector...\nHaar Cascade detector initialized\n  Training on Person IDs: 01, 02, 03, 04, 05, 06, 07, 08, 09, 10\n\nDataset Statistics:\n   Total videos in REAL dir: 363\n   Total videos in FAKE dir: 3068\n  Filtered REAL videos (Persons 01, 02, 03, 04, 05, 06, 07, 08, 09, 10): 128\n  Filtered FAKE videos (Persons 01, 02, 03, 04, 05, 06, 07, 08, 09, 10): 1224\n\n Extracting frames from videos using Haar Cascade...\n","output_type":"stream"},{"name":"stderr","text":"Processing REAL videos: 100%|██████████| 128/128 [22:30<00:00, 10.55s/it]\nProcessing FAKE videos: 100%|██████████| 1224/1224 [3:30:22<00:00, 10.31s/it] \n","output_type":"stream"},{"name":"stdout","text":"\nExtracted 103397 total frames\n\n DATA BALANCING ACTIVE\n   Before: Real=9473, Fake=93924 (Imbalance ratio: 9.91:1)\n   Balancing to: 9473 samples per class...\n   After: Real=9473, Fake=9473 (Perfect 1:1 balance)\n\n======================================================================\n Training MicroArtifactCNN\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|██████████| 69/69 [01:26<00:00,  1.25s/it, loss=0.6901, acc=0.5115]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: Train Loss=0.7047, Train Acc=0.5115 | Val Loss=0.6930, Val Acc=0.5032\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5032)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.7044, acc=0.4988]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: Train Loss=0.7024, Train Acc=0.4988 | Val Loss=0.6922, Val Acc=0.5172\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5172)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.7085, acc=0.5141]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: Train Loss=0.6977, Train Acc=0.5141 | Val Loss=0.6919, Val Acc=0.5232\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5232)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6935, acc=0.5081]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4: Train Loss=0.6978, Train Acc=0.5081 | Val Loss=0.6922, Val Acc=0.5113\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6921, acc=0.5005]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5: Train Loss=0.6971, Train Acc=0.5005 | Val Loss=0.6919, Val Acc=0.5137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6854, acc=0.5114]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6: Train Loss=0.6956, Train Acc=0.5114 | Val Loss=0.6919, Val Acc=0.5148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6971, acc=0.5116]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7: Train Loss=0.6951, Train Acc=0.5116 | Val Loss=0.6926, Val Acc=0.5040\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6718, acc=0.5076]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8: Train Loss=0.6942, Train Acc=0.5076 | Val Loss=0.6917, Val Acc=0.5177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6882, acc=0.5099]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9: Train Loss=0.6947, Train Acc=0.5099 | Val Loss=0.6920, Val Acc=0.5121\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6953, acc=0.5099]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10: Train Loss=0.6943, Train Acc=0.5099 | Val Loss=0.6920, Val Acc=0.5142\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6905, acc=0.5131]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11: Train Loss=0.6940, Train Acc=0.5131 | Val Loss=0.6914, Val Acc=0.5232\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6904, acc=0.5096]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12: Train Loss=0.6941, Train Acc=0.5096 | Val Loss=0.6911, Val Acc=0.5280\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5280)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6942, acc=0.5080]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13: Train Loss=0.6942, Train Acc=0.5080 | Val Loss=0.6912, Val Acc=0.5301\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5301)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6971, acc=0.5079]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14: Train Loss=0.6932, Train Acc=0.5079 | Val Loss=0.6915, Val Acc=0.5211\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6939, acc=0.5110]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15: Train Loss=0.6934, Train Acc=0.5110 | Val Loss=0.6912, Val Acc=0.5227\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6799, acc=0.5176]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16: Train Loss=0.6927, Train Acc=0.5176 | Val Loss=0.6911, Val Acc=0.5261\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.7021, acc=0.5185]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17: Train Loss=0.6936, Train Acc=0.5185 | Val Loss=0.6910, Val Acc=0.5272\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6906, acc=0.5137]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18: Train Loss=0.6929, Train Acc=0.5137 | Val Loss=0.6906, Val Acc=0.5327\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5327)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6952, acc=0.5084]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19: Train Loss=0.6936, Train Acc=0.5084 | Val Loss=0.6906, Val Acc=0.5406\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5406)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6845, acc=0.5195]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20: Train Loss=0.6929, Train Acc=0.5195 | Val Loss=0.6905, Val Acc=0.5433\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5433)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.7088, acc=0.5162]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21: Train Loss=0.6923, Train Acc=0.5162 | Val Loss=0.6905, Val Acc=0.5401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6885, acc=0.5210]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22: Train Loss=0.6922, Train Acc=0.5210 | Val Loss=0.6902, Val Acc=0.5414\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6978, acc=0.5156]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23: Train Loss=0.6926, Train Acc=0.5156 | Val Loss=0.6902, Val Acc=0.5380\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6839, acc=0.5170]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24: Train Loss=0.6921, Train Acc=0.5170 | Val Loss=0.6899, Val Acc=0.5401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6883, acc=0.5156]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25: Train Loss=0.6931, Train Acc=0.5156 | Val Loss=0.6899, Val Acc=0.5406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6959, acc=0.5154]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26: Train Loss=0.6923, Train Acc=0.5154 | Val Loss=0.6897, Val Acc=0.5388\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6858, acc=0.5248]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27: Train Loss=0.6915, Train Acc=0.5248 | Val Loss=0.6896, Val Acc=0.5372\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6857, acc=0.5272]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28: Train Loss=0.6909, Train Acc=0.5272 | Val Loss=0.6895, Val Acc=0.5364\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6854, acc=0.5259]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29: Train Loss=0.6913, Train Acc=0.5259 | Val Loss=0.6894, Val Acc=0.5385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6939, acc=0.5194]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30: Train Loss=0.6915, Train Acc=0.5194 | Val Loss=0.6893, Val Acc=0.5391\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.7046, acc=0.5245]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 31: Train Loss=0.6914, Train Acc=0.5245 | Val Loss=0.6889, Val Acc=0.5441\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5441)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6794, acc=0.5284]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 32: Train Loss=0.6910, Train Acc=0.5284 | Val Loss=0.6890, Val Acc=0.5380\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6724, acc=0.5245]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 33: Train Loss=0.6913, Train Acc=0.5245 | Val Loss=0.6888, Val Acc=0.5401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.7085, acc=0.5266]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 34: Train Loss=0.6915, Train Acc=0.5266 | Val Loss=0.6887, Val Acc=0.5401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6977, acc=0.5259]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 35: Train Loss=0.6914, Train Acc=0.5259 | Val Loss=0.6886, Val Acc=0.5398\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6907, acc=0.5322]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 36: Train Loss=0.6907, Train Acc=0.5322 | Val Loss=0.6886, Val Acc=0.5391\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6984, acc=0.5170]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 37: Train Loss=0.6920, Train Acc=0.5170 | Val Loss=0.6885, Val Acc=0.5404\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6920, acc=0.5284]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 38: Train Loss=0.6919, Train Acc=0.5284 | Val Loss=0.6885, Val Acc=0.5385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6952, acc=0.5240]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 39: Train Loss=0.6911, Train Acc=0.5240 | Val Loss=0.6886, Val Acc=0.5412\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6940, acc=0.5256]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40: Train Loss=0.6908, Train Acc=0.5256 | Val Loss=0.6886, Val Acc=0.5393\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6871, acc=0.5215]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 41: Train Loss=0.6914, Train Acc=0.5215 | Val Loss=0.6886, Val Acc=0.5435\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6981, acc=0.5193]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 42: Train Loss=0.6915, Train Acc=0.5193 | Val Loss=0.6885, Val Acc=0.5449\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5449)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6876, acc=0.5236]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 43: Train Loss=0.6918, Train Acc=0.5236 | Val Loss=0.6885, Val Acc=0.5435\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6830, acc=0.5212]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 44: Train Loss=0.6909, Train Acc=0.5212 | Val Loss=0.6883, Val Acc=0.5470\nSaved best model: MicroArtifactCNN_best.pth (Val Acc: 0.5470)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6880, acc=0.5308]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 45: Train Loss=0.6909, Train Acc=0.5308 | Val Loss=0.6884, Val Acc=0.5451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6958, acc=0.5318]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 46: Train Loss=0.6908, Train Acc=0.5318 | Val Loss=0.6884, Val Acc=0.5404\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.7059, acc=0.5172]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 47: Train Loss=0.6927, Train Acc=0.5172 | Val Loss=0.6884, Val Acc=0.5409\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6930, acc=0.5260]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 48: Train Loss=0.6911, Train Acc=0.5260 | Val Loss=0.6885, Val Acc=0.5404\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.36it/s, loss=0.6934, acc=0.5264]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 49: Train Loss=0.6905, Train Acc=0.5264 | Val Loss=0.6884, Val Acc=0.5451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|██████████| 69/69 [00:50<00:00,  1.37it/s, loss=0.6917, acc=0.5191]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 50: Train Loss=0.6922, Train Acc=0.5191 | Val Loss=0.6884, Val Acc=0.5454\n\n MicroArtifactCNN completed - Best Val Acc: 0.5470\n\n======================================================================\n Training XceptionDeepfake\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|██████████| 69/69 [00:43<00:00,  1.57it/s, loss=0.7095, acc=0.5060]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: Train Loss=0.7033, Train Acc=0.5060 | Val Loss=0.6928, Val Acc=0.5045\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5045)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6659, acc=0.5001]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: Train Loss=0.6993, Train Acc=0.5001 | Val Loss=0.6923, Val Acc=0.5259\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5259)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.6858, acc=0.5059]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: Train Loss=0.6964, Train Acc=0.5059 | Val Loss=0.6921, Val Acc=0.5222\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.6980, acc=0.5040]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4: Train Loss=0.6955, Train Acc=0.5040 | Val Loss=0.6923, Val Acc=0.5174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.32it/s, loss=0.6871, acc=0.5054]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5: Train Loss=0.6951, Train Acc=0.5054 | Val Loss=0.6918, Val Acc=0.5343\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5343)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.7000, acc=0.5102]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6: Train Loss=0.6941, Train Acc=0.5102 | Val Loss=0.6918, Val Acc=0.5317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.7029, acc=0.5154]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7: Train Loss=0.6933, Train Acc=0.5154 | Val Loss=0.6916, Val Acc=0.5235\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6999, acc=0.5197]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8: Train Loss=0.6930, Train Acc=0.5197 | Val Loss=0.6911, Val Acc=0.5367\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5367)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.6890, acc=0.5215]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9: Train Loss=0.6922, Train Acc=0.5215 | Val Loss=0.6911, Val Acc=0.5443\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5443)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6877, acc=0.5221]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10: Train Loss=0.6924, Train Acc=0.5221 | Val Loss=0.6906, Val Acc=0.5356\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.7036, acc=0.5188]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11: Train Loss=0.6924, Train Acc=0.5188 | Val Loss=0.6908, Val Acc=0.5301\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6961, acc=0.5230]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12: Train Loss=0.6921, Train Acc=0.5230 | Val Loss=0.6900, Val Acc=0.5425\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6846, acc=0.5212]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13: Train Loss=0.6920, Train Acc=0.5212 | Val Loss=0.6905, Val Acc=0.5303\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6796, acc=0.5234]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14: Train Loss=0.6911, Train Acc=0.5234 | Val Loss=0.6907, Val Acc=0.5322\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6921, acc=0.5260]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15: Train Loss=0.6916, Train Acc=0.5260 | Val Loss=0.6898, Val Acc=0.5369\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6940, acc=0.5300]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16: Train Loss=0.6914, Train Acc=0.5300 | Val Loss=0.6894, Val Acc=0.5414\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6997, acc=0.5298]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17: Train Loss=0.6908, Train Acc=0.5298 | Val Loss=0.6888, Val Acc=0.5446\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5446)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.32it/s, loss=0.6825, acc=0.5274]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18: Train Loss=0.6905, Train Acc=0.5274 | Val Loss=0.6887, Val Acc=0.5480\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5480)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6765, acc=0.5393]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19: Train Loss=0.6892, Train Acc=0.5393 | Val Loss=0.6888, Val Acc=0.5422\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.6932, acc=0.5381]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20: Train Loss=0.6891, Train Acc=0.5381 | Val Loss=0.6877, Val Acc=0.5528\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5528)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.7019, acc=0.5366]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21: Train Loss=0.6893, Train Acc=0.5366 | Val Loss=0.6873, Val Acc=0.5544\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5544)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.34it/s, loss=0.6788, acc=0.5369]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22: Train Loss=0.6885, Train Acc=0.5369 | Val Loss=0.6881, Val Acc=0.5533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.6951, acc=0.5407]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23: Train Loss=0.6882, Train Acc=0.5407 | Val Loss=0.6874, Val Acc=0.5525\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.32it/s, loss=0.6931, acc=0.5435]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24: Train Loss=0.6875, Train Acc=0.5435 | Val Loss=0.6858, Val Acc=0.5522\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6908, acc=0.5389]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25: Train Loss=0.6893, Train Acc=0.5389 | Val Loss=0.6858, Val Acc=0.5567\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5567)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.32it/s, loss=0.6668, acc=0.5428]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26: Train Loss=0.6873, Train Acc=0.5428 | Val Loss=0.6858, Val Acc=0.5620\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5620)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6794, acc=0.5445]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27: Train Loss=0.6879, Train Acc=0.5445 | Val Loss=0.6858, Val Acc=0.5620\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.6873, acc=0.5420]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28: Train Loss=0.6862, Train Acc=0.5420 | Val Loss=0.6859, Val Acc=0.5646\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5646)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6727, acc=0.5414]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29: Train Loss=0.6867, Train Acc=0.5414 | Val Loss=0.6842, Val Acc=0.5668\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5668)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6878, acc=0.5476]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30: Train Loss=0.6855, Train Acc=0.5476 | Val Loss=0.6857, Val Acc=0.5691\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5691)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6813, acc=0.5423]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 31: Train Loss=0.6863, Train Acc=0.5423 | Val Loss=0.6830, Val Acc=0.5778\nSaved best model: XceptionDeepfake_best.pth (Val Acc: 0.5778)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6765, acc=0.5525]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 32: Train Loss=0.6843, Train Acc=0.5525 | Val Loss=0.6847, Val Acc=0.5641\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6562, acc=0.5530]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 33: Train Loss=0.6842, Train Acc=0.5530 | Val Loss=0.6836, Val Acc=0.5683\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6821, acc=0.5468]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 34: Train Loss=0.6853, Train Acc=0.5468 | Val Loss=0.6828, Val Acc=0.5697\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6544, acc=0.5501]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 35: Train Loss=0.6853, Train Acc=0.5501 | Val Loss=0.6823, Val Acc=0.5726\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6573, acc=0.5515]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 36: Train Loss=0.6850, Train Acc=0.5515 | Val Loss=0.6822, Val Acc=0.5739\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|██████████| 69/69 [00:30<00:00,  2.30it/s, loss=0.6866, acc=0.5530]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 37: Train Loss=0.6842, Train Acc=0.5530 | Val Loss=0.6831, Val Acc=0.5686\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.32it/s, loss=0.6835, acc=0.5573]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 38: Train Loss=0.6831, Train Acc=0.5573 | Val Loss=0.6822, Val Acc=0.5720\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.6842, acc=0.5538]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 39: Train Loss=0.6845, Train Acc=0.5538 | Val Loss=0.6829, Val Acc=0.5718\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.7150, acc=0.5496]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40: Train Loss=0.6856, Train Acc=0.5496 | Val Loss=0.6824, Val Acc=0.5728\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6780, acc=0.5547]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 41: Train Loss=0.6839, Train Acc=0.5547 | Val Loss=0.6826, Val Acc=0.5744\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6969, acc=0.5549]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 42: Train Loss=0.6834, Train Acc=0.5549 | Val Loss=0.6830, Val Acc=0.5723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6811, acc=0.5586]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 43: Train Loss=0.6825, Train Acc=0.5586 | Val Loss=0.6823, Val Acc=0.5734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6706, acc=0.5585]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 44: Train Loss=0.6834, Train Acc=0.5585 | Val Loss=0.6832, Val Acc=0.5720\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6953, acc=0.5590]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 45: Train Loss=0.6823, Train Acc=0.5590 | Val Loss=0.6819, Val Acc=0.5736\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6750, acc=0.5584]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 46: Train Loss=0.6830, Train Acc=0.5584 | Val Loss=0.6820, Val Acc=0.5728\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.30it/s, loss=0.6937, acc=0.5604]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 47: Train Loss=0.6823, Train Acc=0.5604 | Val Loss=0.6822, Val Acc=0.5741\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.31it/s, loss=0.6800, acc=0.5504]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 48: Train Loss=0.6838, Train Acc=0.5504 | Val Loss=0.6826, Val Acc=0.5723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6994, acc=0.5546]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 49: Train Loss=0.6842, Train Acc=0.5546 | Val Loss=0.6826, Val Acc=0.5728\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|██████████| 69/69 [00:29<00:00,  2.33it/s, loss=0.6734, acc=0.5575]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 50: Train Loss=0.6822, Train Acc=0.5575 | Val Loss=0.6820, Val Acc=0.5728\n\n XceptionDeepfake completed - Best Val Acc: 0.5778\n\n======================================================================\n Training ResNet50Deepfake\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|██████████| 69/69 [00:40<00:00,  1.69it/s, loss=0.6988, acc=0.5156]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: Train Loss=0.7020, Train Acc=0.5156 | Val Loss=0.6927, Val Acc=0.5103\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5103)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.44it/s, loss=0.7037, acc=0.5104]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: Train Loss=0.6953, Train Acc=0.5104 | Val Loss=0.6934, Val Acc=0.5129\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5129)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.45it/s, loss=0.6901, acc=0.5195]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: Train Loss=0.6939, Train Acc=0.5195 | Val Loss=0.6906, Val Acc=0.5372\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5372)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.7029, acc=0.5300]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4: Train Loss=0.6925, Train Acc=0.5300 | Val Loss=0.6933, Val Acc=0.5441\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5441)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.6705, acc=0.5314]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5: Train Loss=0.6911, Train Acc=0.5314 | Val Loss=0.6860, Val Acc=0.5554\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5554)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.6742, acc=0.5466]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6: Train Loss=0.6884, Train Acc=0.5466 | Val Loss=0.6789, Val Acc=0.5599\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5599)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.6400, acc=0.5481]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7: Train Loss=0.6854, Train Acc=0.5481 | Val Loss=0.6733, Val Acc=0.5763\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5763)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.6848, acc=0.5515]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8: Train Loss=0.6860, Train Acc=0.5515 | Val Loss=0.6740, Val Acc=0.5852\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.5852)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.6675, acc=0.5678]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9: Train Loss=0.6799, Train Acc=0.5678 | Val Loss=0.6606, Val Acc=0.6029\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.6029)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.45it/s, loss=0.6717, acc=0.5850]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10: Train Loss=0.6715, Train Acc=0.5850 | Val Loss=0.6444, Val Acc=0.6298\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.6298)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.48it/s, loss=0.6237, acc=0.6216]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11: Train Loss=0.6457, Train Acc=0.6216 | Val Loss=1.0131, Val Acc=0.5274\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.5386, acc=0.6443]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12: Train Loss=0.6162, Train Acc=0.6443 | Val Loss=0.5469, Val Acc=0.7058\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.7058)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.5045, acc=0.6933]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13: Train Loss=0.5597, Train Acc=0.6933 | Val Loss=0.5353, Val Acc=0.7427\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.7427)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.5203, acc=0.7110]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14: Train Loss=0.5325, Train Acc=0.7110 | Val Loss=0.4680, Val Acc=0.7522\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.7522)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.45it/s, loss=0.4971, acc=0.7375]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15: Train Loss=0.4915, Train Acc=0.7375 | Val Loss=0.3982, Val Acc=0.7939\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.7939)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.4956, acc=0.7394]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16: Train Loss=0.4801, Train Acc=0.7394 | Val Loss=0.3687, Val Acc=0.8179\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8179)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.5371, acc=0.7495]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17: Train Loss=0.4523, Train Acc=0.7495 | Val Loss=0.3447, Val Acc=0.8256\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8256)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3860, acc=0.7677]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18: Train Loss=0.4283, Train Acc=0.7677 | Val Loss=0.3512, Val Acc=0.8156\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.44it/s, loss=0.3914, acc=0.7720]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19: Train Loss=0.4154, Train Acc=0.7720 | Val Loss=0.3389, Val Acc=0.8256\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.3671, acc=0.7734]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20: Train Loss=0.4130, Train Acc=0.7734 | Val Loss=0.4286, Val Acc=0.7834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.2794, acc=0.7824]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21: Train Loss=0.4028, Train Acc=0.7824 | Val Loss=0.3394, Val Acc=0.8208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.3737, acc=0.7910]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22: Train Loss=0.3792, Train Acc=0.7910 | Val Loss=0.3424, Val Acc=0.8264\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8264)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3689, acc=0.7924]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23: Train Loss=0.3803, Train Acc=0.7924 | Val Loss=0.2772, Val Acc=0.8570\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8570)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.4801, acc=0.8010]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24: Train Loss=0.3689, Train Acc=0.8010 | Val Loss=0.3465, Val Acc=0.8206\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.45it/s, loss=0.5733, acc=0.7984]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25: Train Loss=0.3614, Train Acc=0.7984 | Val Loss=0.4295, Val Acc=0.7850\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.45it/s, loss=0.3437, acc=0.8038]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26: Train Loss=0.3588, Train Acc=0.8038 | Val Loss=0.2589, Val Acc=0.8631\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8631)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.2778, acc=0.8096]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27: Train Loss=0.3390, Train Acc=0.8096 | Val Loss=0.2502, Val Acc=0.8678\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8678)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3240, acc=0.8135]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28: Train Loss=0.3424, Train Acc=0.8135 | Val Loss=0.2484, Val Acc=0.8689\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8689)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3234, acc=0.8162]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29: Train Loss=0.3345, Train Acc=0.8162 | Val Loss=0.2684, Val Acc=0.8609\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3796, acc=0.8173]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30: Train Loss=0.3252, Train Acc=0.8173 | Val Loss=0.2379, Val Acc=0.8739\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8739)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.4572, acc=0.8223]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 31: Train Loss=0.3211, Train Acc=0.8223 | Val Loss=0.2796, Val Acc=0.8580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.3665, acc=0.8164]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 32: Train Loss=0.3254, Train Acc=0.8164 | Val Loss=0.2461, Val Acc=0.8691\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.3392, acc=0.8249]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 33: Train Loss=0.3085, Train Acc=0.8249 | Val Loss=0.2235, Val Acc=0.8839\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8839)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.45it/s, loss=0.1936, acc=0.8272]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 34: Train Loss=0.3088, Train Acc=0.8272 | Val Loss=0.2228, Val Acc=0.8805\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.2131, acc=0.8331]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 35: Train Loss=0.3017, Train Acc=0.8331 | Val Loss=0.2379, Val Acc=0.8802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.3779, acc=0.8296]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 36: Train Loss=0.3028, Train Acc=0.8296 | Val Loss=0.2161, Val Acc=0.8879\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8879)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s, loss=0.2992, acc=0.8378]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 37: Train Loss=0.2900, Train Acc=0.8378 | Val Loss=0.2179, Val Acc=0.8850\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.5260, acc=0.8359]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 38: Train Loss=0.2937, Train Acc=0.8359 | Val Loss=0.2184, Val Acc=0.8868\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.2798, acc=0.8320]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 39: Train Loss=0.2930, Train Acc=0.8320 | Val Loss=0.2423, Val Acc=0.8768\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.2881, acc=0.8376]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40: Train Loss=0.2885, Train Acc=0.8376 | Val Loss=0.2353, Val Acc=0.8810\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.44it/s, loss=0.2444, acc=0.8376]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 41: Train Loss=0.2859, Train Acc=0.8376 | Val Loss=0.2121, Val Acc=0.8868\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.3081, acc=0.8340]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 42: Train Loss=0.2867, Train Acc=0.8340 | Val Loss=0.2135, Val Acc=0.8839\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.2263, acc=0.8440]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 43: Train Loss=0.2778, Train Acc=0.8440 | Val Loss=0.2117, Val Acc=0.8871\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.3288, acc=0.8421]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 44: Train Loss=0.2739, Train Acc=0.8421 | Val Loss=0.2131, Val Acc=0.8860\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.42it/s, loss=0.3100, acc=0.8390]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 45: Train Loss=0.2808, Train Acc=0.8390 | Val Loss=0.2102, Val Acc=0.8871\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.3311, acc=0.8427]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 46: Train Loss=0.2826, Train Acc=0.8427 | Val Loss=0.2096, Val Acc=0.8873\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.43it/s, loss=0.4145, acc=0.8406]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 47: Train Loss=0.2806, Train Acc=0.8406 | Val Loss=0.2110, Val Acc=0.8850\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.46it/s, loss=0.2397, acc=0.8395]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 48: Train Loss=0.2785, Train Acc=0.8395 | Val Loss=0.2100, Val Acc=0.8881\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8881)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.2884, acc=0.8425]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 49: Train Loss=0.2781, Train Acc=0.8425 | Val Loss=0.2089, Val Acc=0.8884\nSaved best model: ResNet50Deepfake_best.pth (Val Acc: 0.8884)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|██████████| 69/69 [00:20<00:00,  3.41it/s, loss=0.2824, acc=0.8414]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 50: Train Loss=0.2797, Train Acc=0.8414 | Val Loss=0.2086, Val Acc=0.8881\n\n ResNet50Deepfake completed - Best Val Acc: 0.8884\n\n======================================================================\n Training EfficientNetB0\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|██████████| 69/69 [00:45<00:00,  1.51it/s, loss=0.7064, acc=0.5085]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: Train Loss=0.7152, Train Acc=0.5085 | Val Loss=0.6931, Val Acc=0.5079\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5079)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6799, acc=0.5054]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: Train Loss=0.7116, Train Acc=0.5054 | Val Loss=0.6930, Val Acc=0.5063\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.61it/s, loss=0.6688, acc=0.5071]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: Train Loss=0.7101, Train Acc=0.5071 | Val Loss=0.6943, Val Acc=0.5005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7444, acc=0.5050]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4: Train Loss=0.7115, Train Acc=0.5050 | Val Loss=0.6932, Val Acc=0.5037\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.6807, acc=0.5030]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5: Train Loss=0.7089, Train Acc=0.5030 | Val Loss=0.6927, Val Acc=0.5317\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5317)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7257, acc=0.5098]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6: Train Loss=0.7047, Train Acc=0.5098 | Val Loss=0.6931, Val Acc=0.5016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.6602, acc=0.5138]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7: Train Loss=0.7052, Train Acc=0.5138 | Val Loss=0.6932, Val Acc=0.5040\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.52it/s, loss=0.7016, acc=0.5051]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8: Train Loss=0.7045, Train Acc=0.5051 | Val Loss=0.6937, Val Acc=0.5018\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7030, acc=0.5061]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9: Train Loss=0.7037, Train Acc=0.5061 | Val Loss=0.6933, Val Acc=0.5058\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.7110, acc=0.5086]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10: Train Loss=0.7043, Train Acc=0.5086 | Val Loss=0.6917, Val Acc=0.5330\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5330)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.7038, acc=0.5134]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11: Train Loss=0.7022, Train Acc=0.5134 | Val Loss=0.6918, Val Acc=0.5282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.6800, acc=0.5148]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12: Train Loss=0.7011, Train Acc=0.5148 | Val Loss=0.6923, Val Acc=0.5148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.6837, acc=0.5115]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13: Train Loss=0.7019, Train Acc=0.5115 | Val Loss=0.6909, Val Acc=0.5317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6817, acc=0.5217]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14: Train Loss=0.6993, Train Acc=0.5217 | Val Loss=0.6907, Val Acc=0.5343\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5343)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.60it/s, loss=0.7203, acc=0.5216]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15: Train Loss=0.6996, Train Acc=0.5216 | Val Loss=0.6901, Val Acc=0.5420\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5420)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7098, acc=0.5258]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16: Train Loss=0.6992, Train Acc=0.5258 | Val Loss=0.6911, Val Acc=0.5237\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.6789, acc=0.5324]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17: Train Loss=0.6955, Train Acc=0.5324 | Val Loss=0.6918, Val Acc=0.5282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7002, acc=0.5232]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18: Train Loss=0.6979, Train Acc=0.5232 | Val Loss=0.6905, Val Acc=0.5346\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.7128, acc=0.5336]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19: Train Loss=0.6967, Train Acc=0.5336 | Val Loss=0.6903, Val Acc=0.5393\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.7068, acc=0.5240]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20: Train Loss=0.6966, Train Acc=0.5240 | Val Loss=0.6889, Val Acc=0.5375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.55it/s, loss=0.7553, acc=0.5288]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21: Train Loss=0.6972, Train Acc=0.5288 | Val Loss=0.6886, Val Acc=0.5385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7283, acc=0.5259]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22: Train Loss=0.6963, Train Acc=0.5259 | Val Loss=0.6896, Val Acc=0.5462\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5462)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.7027, acc=0.5302]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23: Train Loss=0.6926, Train Acc=0.5302 | Val Loss=0.6886, Val Acc=0.5472\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5472)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.7012, acc=0.5299]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24: Train Loss=0.6953, Train Acc=0.5299 | Val Loss=0.6879, Val Acc=0.5464\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6972, acc=0.5352]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25: Train Loss=0.6933, Train Acc=0.5352 | Val Loss=0.6874, Val Acc=0.5430\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7053, acc=0.5331]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26: Train Loss=0.6941, Train Acc=0.5331 | Val Loss=0.6882, Val Acc=0.5414\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.53it/s, loss=0.6837, acc=0.5420]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27: Train Loss=0.6914, Train Acc=0.5420 | Val Loss=0.6871, Val Acc=0.5485\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5485)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.61it/s, loss=0.6993, acc=0.5412]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28: Train Loss=0.6930, Train Acc=0.5412 | Val Loss=0.6868, Val Acc=0.5475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.61it/s, loss=0.6590, acc=0.5387]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29: Train Loss=0.6920, Train Acc=0.5387 | Val Loss=0.6857, Val Acc=0.5501\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5501)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6770, acc=0.5375]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30: Train Loss=0.6919, Train Acc=0.5375 | Val Loss=0.6859, Val Acc=0.5478\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.55it/s, loss=0.6957, acc=0.5410]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 31: Train Loss=0.6901, Train Acc=0.5410 | Val Loss=0.6865, Val Acc=0.5470\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.60it/s, loss=0.7090, acc=0.5429]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 32: Train Loss=0.6897, Train Acc=0.5429 | Val Loss=0.6869, Val Acc=0.5467\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.6675, acc=0.5410]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 33: Train Loss=0.6904, Train Acc=0.5410 | Val Loss=0.6880, Val Acc=0.5480\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.55it/s, loss=0.6800, acc=0.5443]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 34: Train Loss=0.6893, Train Acc=0.5443 | Val Loss=0.6867, Val Acc=0.5515\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5515)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.6651, acc=0.5476]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 35: Train Loss=0.6893, Train Acc=0.5476 | Val Loss=0.6854, Val Acc=0.5522\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5522)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.54it/s, loss=0.6753, acc=0.5441]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 36: Train Loss=0.6898, Train Acc=0.5441 | Val Loss=0.6851, Val Acc=0.5520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.6901, acc=0.5435]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 37: Train Loss=0.6901, Train Acc=0.5435 | Val Loss=0.6851, Val Acc=0.5520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.6623, acc=0.5473]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 38: Train Loss=0.6874, Train Acc=0.5473 | Val Loss=0.6846, Val Acc=0.5517\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.6609, acc=0.5434]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 39: Train Loss=0.6887, Train Acc=0.5434 | Val Loss=0.6843, Val Acc=0.5536\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5536)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.7081, acc=0.5410]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40: Train Loss=0.6915, Train Acc=0.5410 | Val Loss=0.6849, Val Acc=0.5536\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.59it/s, loss=0.7114, acc=0.5494]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 41: Train Loss=0.6885, Train Acc=0.5494 | Val Loss=0.6836, Val Acc=0.5554\nSaved best model: EfficientNetB0_best.pth (Val Acc: 0.5554)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6920, acc=0.5439]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 42: Train Loss=0.6893, Train Acc=0.5439 | Val Loss=0.6837, Val Acc=0.5551\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.7214, acc=0.5418]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 43: Train Loss=0.6898, Train Acc=0.5418 | Val Loss=0.6847, Val Acc=0.5533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.6917, acc=0.5500]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 44: Train Loss=0.6875, Train Acc=0.5500 | Val Loss=0.6834, Val Acc=0.5549\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.54it/s, loss=0.6626, acc=0.5430]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 45: Train Loss=0.6884, Train Acc=0.5430 | Val Loss=0.6838, Val Acc=0.5533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6942, acc=0.5435]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 46: Train Loss=0.6886, Train Acc=0.5435 | Val Loss=0.6850, Val Acc=0.5528\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.56it/s, loss=0.6499, acc=0.5470]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 47: Train Loss=0.6874, Train Acc=0.5470 | Val Loss=0.6845, Val Acc=0.5509\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.57it/s, loss=0.7112, acc=0.5475]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 48: Train Loss=0.6882, Train Acc=0.5475 | Val Loss=0.6845, Val Acc=0.5533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.54it/s, loss=0.6912, acc=0.5463]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 49: Train Loss=0.6882, Train Acc=0.5463 | Val Loss=0.6839, Val Acc=0.5485\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|██████████| 69/69 [00:19<00:00,  3.58it/s, loss=0.6973, acc=0.5488]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 50: Train Loss=0.6882, Train Acc=0.5488 | Val Loss=0.6840, Val Acc=0.5525\n\n EfficientNetB0 completed - Best Val Acc: 0.5554\n\n======================================================================\nTRAINING SUMMARY\n======================================================================\nMicroArtifactCNN: Best Val Acc = 0.5470\nXceptionDeepfake: Best Val Acc = 0.5778\nResNet50Deepfake: Best Val Acc = 0.8884\nEfficientNetB0: Best Val Acc = 0.5554\n\n All models saved as .pth files (PyTorch format)\n Models are ready for fine-tuning and ensemble inference\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}